# 部署文档

## 服务器集群

hostname | IP | user | password | path | os
---------|-------------------------------------------------
spark01 |  192.168.218.201  | hadoop | java | /hadoop | Centos7
--------|-----------------------------------------------
spark02 |  192.168.218.202  | hadoop | java | /hadoop  | Centos7
--------|------------------------------------------------
spark03 |192.168.218.201| hadoop | java | /hadoop | Centos7

## 集群规划

spark01 | spark02 | spark03
----------------------------------------------|
namenode | namenode | 
---------------------------------------------|
datanode | datanode | datanode 
----------------------------------------------|
zookeeper | zookeeper | zookeeper 
----------------------------------------------|
ResourceManage | ResourceManage | 
----------------------------------------------|
NodeManage | NodeManage | NodeManage 

## 组件版本

组件 | 版本 | 下载地址
----------------------------------------------|
CentOS7 | CentOS-7-x86_64-DVD-1908.iso | [linux服务器下载地址][1]
----------------------------------------------|
JDK | jdk1.8.0_141 | [Jdk][2]
----------------------------------------------|
Zookeeper | zookeeper-3.4.5-cdh5.14.2 | [zookeeper下载地址][3]
----------------------------------------------|
Hadoop | hadoop-2.6.0-cdh5.14.2 | [hadoop下载地址][4]

## 配置服务器

从此步开始，开始进行服务器基础环境配置。

### 将集群中所有的机器hostname ip 映射，添加到/etc/hosts

映射后集群间将不在需要使用ip

```shell
    192.168.218.201 spark01
    192.168.218.202 spark02
    192.168.218.203 spark03
```

### 关闭防火墙

关闭防火墙主要是为了集群机器间的通信

```shell
# 关闭防火墙
systemctl disable firewalld.service

# 查看防火墙状态
systemctl status firewalld.service
# 已关闭将输出 Active: inactive (dead)
```

### 配置网卡及主机名

编辑文件`vim /etc/sysconfig/network-scripts/ifcfg-eth0` ，写入如下配置：

```shell
TYPE=Ethernet
PROXY_METHOD=none
BROWSER_ONLY=no
DEFROUTE=yes
IPV4_FAILURE_FATAL=no
IPV6INIT=yes
IPV6_AUTOCONF=yes
IPV6_DEFROUTE=yes
IPV6_FAILURE_FATAL=no
IPV6_ADDR_GEN_MODE=stable-privacy
NAME=eth0
DEVICE=eth0
IPV6_PRIVACY=no
PREFIX=24

## 下边的几项配置是修改的
#UUID=f22334e3-05d1-450e-a50a-1da9f5f27915
ONBOOT=yes
BOOTPROTO=static
IPADDR=192.168.218.110 # 当前机器的ip
GATEWAY=192.168.218.2 # 路由 要求网段一直 218 数字与ip的一样
DNS1=192.168.218.2 # 同上
```

配置主机名编辑文件`/etc/hostname`，添加下列

```shell
spark01
```

配置完成后，重启网络服务。

```shell
service network restart
```

### 同步阿里云服务器时间

```shell
# 安装ntpdate
yum -y install ntpdate


# 安装完成执行命令
crontab -e


# 此时进入文本编辑模式  使用 i 插入下列命令
*/1 * * * * /usr/sbin/ntpdate time1.aliyun.com
# 填写完成后，输入 :wq 保存退出
```

### 添加用户

按步骤执行以下命令

```shell
# 添加用户组
groupadd hadoop


# 创建用户并添加到hadoop组中
useradd -g hadoop hadoop


# 使用id命令查看hadoop用户组和hadoop用户创建是否成功
id hadoop
# 正常输出 uid=1000(hadoop) gid=1000(hadoop) groups=1000(hadoop)


# 设置hadoop用户密码为hadoop
passwd hadoop

```

### 创建应用安装包以及数据存储目录

```java
 mkdir -p /hadoop/soft     # 软件压缩包存放目录
 mkdir -p /hadoop/install  # 软件解压后存放目录
 mkdir -p /hadoop/datadir  # 各应用的数据存放目录
 chown -R hadoop:hadoop /hadoop    # 将文件夹权限更改为hadoop用户
```

### 上传安装包以及解压

#### 上传

根据下载链接将需要组件下载到宿主机,由宿主机上传到虚拟机中

**注意**：这里上传时要使用`hadoop`用户，不然还需要更改文件所属用户！！

上传路径为 `/hadoop/soft`~

至于用什么方式，sftp、scp或其他工具都可！

#### 解压

使用`hadoop`用户登录，解压命令直接解压即可

**注意**：一定要用`hadoop`用户！！！

```shell
tar -xzvf hadoop-2.6.0-cdh5.14.2.tar.gz -C /hadoopadmin/install/
```

### 配置jdk

jdk可选择配置全局，也可以选择配置只针对`hadoop`用户。

这里我选择配置只针对`hadoop`用户~

命令 `vim ~/.bash_profile`

```shell
export JAVA_HOME=/hadoopadmin/install/jdk1.8.0_141

PATH=$PATH:$HOME/bin:$JAVA_HOME/bin
```

修改完成使用命令 `source ~/.bash_profile`，更新用户环境变量。

**验证环境**：

```shell
java -verison

# 正常输出。jdk版本
# 错误输出 找不到命令
```

## 配置zookeeper

zookeeper的配置较为简单,只需要添加两个文件即可

第一个文件 zoo.cfg，命令 `vim zoo.cfg`

```shell
tickTime=2000
initLimit=10
syncLimit=5
clientPort=2181

# 路径需要根据你的真实情况进行修改
dataDir=/hadoopadmin/datadir/zookeeper/
# 只修改你的主机hostname就可以，我这里三台机器命名为，`spark01`、`spark02`、`spark03`
server.1=spark01:2888:3888
server.2=spark02:2888:3888
server.3=spark03:2888:3888

```

第二个文件 myid，进入第一个配置文件中`dataDir`配置的目录，命令 `vim myid`，添加 `1`，即可（**这块每台机器不一样，在我们克隆虚拟机镜像后
需要手动将其修改！！稍后介绍**）。

## 配置hadoop

**hadoop的配置文件不需要区分节点**，也就是说每个几点的配置文件都是相同的，所以我们在克隆虚拟机镜像前先将其配置好，
这样在克隆镜像后尽量最小的配置文件改动！

### 第一步 配置环境变量（参考jdk配置）

```shell
export HADOOP_HOME=/hadoopadmin/install/hadoop-2.6.0-cdh5.14.2

PATH=$PATH:$HOME/bin:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
```

### 第二步 配置xml

需要修改的xml一共四个，都存放在`/hadoopadmin/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/`目录下，`core-site.xml`、`hdfs-site.xml`、`yarn-site.xml`、`mapred-site.xml`。

#### core-site.xml

```xml
<configuration>
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://spark01:8020</value>
    </property>
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/hadoop/datadir/hadoop/tempDatas</value>
    </property>
    <!--  缓冲区大小，实际工作中根据服务器性能动态调整 -->
    <property>
        <name>io.file.buffer.size</name>
        <value>4096</value>
    </property>
    <property>
        <name>fs.trash.interval</name>
        <value>10080</value>
        <description>检查点被删除后的分钟数。 如果为零，垃圾桶功能将被禁用。 
        该选项可以在服务器和客户端上配置。 如果垃圾箱被禁用服务器端，则检查客户端配置。 
        如果在服务器端启用垃圾箱，则会使用服务器上配置的值，并忽略客户端配置值。</description>
    </property>

    <property>
        <name>fs.trash.checkpoint.interval</name>
        <value>0</value>
        <description>垃圾检查点之间的分钟数。 应该小于或等于fs.trash.interval。 
        如果为零，则将该值设置为fs.trash.interval的值。 每次检查指针运行时，
        它都会从当前创建一个新的检查点，并删除比fs.trash.interval更早创建的检查点。</description>
    </property>
</configuration>
```

#### hdfs-site.xml

```xml
<configuration>
    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node01:50090</value>
    </property>
    <property>
        <name>dfs.namenode.http-address</name>
        <value>node01:50070</value>
    </property>
    <property>
        <name>dfs.namenode.name.dir</name>
        <value>file:///hadoop/datadir/hadoop/namenodeDatas</value>
    </property>
    <!--  定义dataNode数据存储的节点位置，实际工作中，一般先确定磁盘的挂载目录，然后多个目录用，进行分割  -->
    <property>
        <name>dfs.datanode.data.dir</name>
        <value>file:///hadoop/datadir/hadoop/datanodeDatas</value>
    </property>

    <property>
        <name>dfs.namenode.edits.dir</name>
        <value>file:///hadoop/datadir/hadoop/dfs/nn/edits</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>file:///hadoop/datadir/hadoop/dfs/snn/name</value>
    </property>
    <property>
        <name>dfs.namenode.checkpoint.edits.dir</name>
        <value>file:///hadoop/datadir/hadoop/dfs/nn/snn/edits</value>
    </property>
    <property>
        <name>dfs.replication</name>
        <value>3</value>
    </property>
    <property>
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
</configuration>
```

#### yarn-site.xml

```xml
<configuration>
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>node01</value>
    </property>
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
    <property>
            <name>yarn.log.server.url</name>
            <value>http://node01:19888/jobhistory/logs</value>
    </property>
    <!--多长时间聚合删除一次日志 此处-->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>2592000</value><!--30 day-->
    </property>
    <!--时间在几秒钟内保留用户日志。只适用于如果日志聚合是禁用的-->
    <property>
        <name>yarn.nodemanager.log.retain-seconds</name>
        <value>604800</value><!--7 day-->
    </property>
    <!--指定文件压缩类型用于压缩汇总日志-->
    <property>
        <name>yarn.nodemanager.log-aggregation.compression-type</name>
        <value>gz</value>
    </property>
    <!-- nodemanager本地文件存储目录-->
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>/hadoop/datadir/hadoop/yarn/local</value>
    </property>
    <!-- resourceManager  保存最大的任务完成个数 -->
    <property>
        <name>yarn.resourcemanager.max-completed-applications</name>
        <value>1000</value>
    </property>
</configuration>
```

#### mapred-site.xml

```xml
<!--指定运行mapreduce的环境是yarn -->
<configuration>
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>
    <property>
        <name>mapreduce.job.ubertask.enable</name>
        <value>true</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node01:10020</value>
    </property>
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>node01:19888</value>
    </property>
</configuration>
```

### 更改slaves

`slaves`文件同样存在`/hadoopadmin/install/hadoop-2.6.0-cdh5.14.2/etc/hadoop/`目录中，

```shell
# vi slaves
#将localhost这一行删除掉，添加下边三个节点
node01
node02
node03
```

### 手动创建hadoop所需数据目录

下边给出命令，直接整体复制执行即可，若你修改了路径，则需要对应的修改。

```shell
mkdir -p /hadoop/datadir/hadoop/tempDatas
mkdir -p /hadoop/datadir/hadoop/namenodeDatas
mkdir -p /hadoop/datadir/hadoop/datanodeDatas
mkdir -p /hadoop/datadir/hadoop/dfs/nn/edits
mkdir -p /hadoop/datadir/hadoop/dfs/snn/name
mkdir -p /hadoop/datadir/hadoop/dfs/nn/snn/edits
mkdir -p /hadoop/datadir/hadoop/yarn/local
```

**注意**：有的同学这块没有注意到上边提到的`hadoop`用户的读写权限，或者用了root创建，导致目录无法写入的异常。该路径一定要属于`hadoop`用户！！！

## 复制虚拟机镜像

这一步直接关机完整克隆就好了

**注意**：有的同学复制镜像的同时把虚拟机的mac地址也复制了，这样将会导致其他两台启动后无法使用，若mac地址相同，
那么重新生成一个mac地址。

## 启动每一个节点

启动每一台虚拟机！

## 配置免密登录

Linux免密登录，本质上是使用了`公钥登录`。原理很简单，就是用户将自己的`公钥`储存在远程主机上。登录的时候，远程主机会向用户发送一段随机字符串，用户用自己的`私钥`加密后，再发回来。远程主机用事先储存的`公钥`进行解密，如果成功，就证明用户是可信的，直接允许登录shell，不再要求密码。

**注意**：以下命令要在每一台机器上都执行~~~

```shell
## 生成密钥
## 期间需要输入几次回车，直接回车即可
ssh-keygen -t rsa

## 发送自己的公钥到每一台机器上，包括自己本身
## 由于每条命令都需要输入对方的密码，所以要一条一条的执行！！！
ssh-copy-id -i ~/.ssh/id_rsa.pub node01
ssh-copy-id -i ~/.ssh/id_rsa.pub node02
ssh-copy-id -i ~/.ssh/id_rsa.pub node03
```

**注意**：一定要验证是否成功，在每台机器上相互`ssh`不需要密码就能登录，那么就说明免密登录配置成功！！！

```shell
ssh spark01
```

## zookeeper启动

### 手动启动每一台节点

**注意**：在启动前，我们要把刚刚的zookeeper配置中myid更改一下，才可以启动！！!

不同的机器对应不同的myid，从下边配置中获取,node01对应1，以此类推。

```shell
server.1=node01:2888:3888
server.2=node02:2888:3888
server.3=node03:2888:3888
```

```shell
# 启动zk
/hadoop/install/zookeeper-3.4.5-cdh5.14.2/bin/zkServer.sh start
# 检查状态
/hadoop/install/zookeeper-3.4.5-cdh5.14.2/bin/zkServer.sh status
```

### 脚本启动所有节点

```shell
#!/bin/bash --login

zk=$1
command=$2
coms=(start status stop)
path=$3

start(){
    echo "$1 zookeeper on $2"
    ssh -l hadoopadmin $2  "$path $1"
    #ssh -l hadoopadmin $zookeeper "java -version"
}

if [ "$zk" == "" ] || [ "$command" == "" ];then
        echo "usage：'node01 node02 node03'  [start status stop]"
        exit 0
fi

# 判断是否为支持的命令
for c in $coms
do
        if [ "$command" != "$c" ];then
                echo "当前只支持：[start status stop]命令"
                exit 0
        fi
done

if [ "$command" != "" ];then
    for zookeeper in $zk
    do
        start $command $zookeeper
    done
    else
        echo "请输入正确命令"
        echo "usage: 'node01 node02 node03' [start status stop]"
fi
```

启动只需要在主节点执行脚本即可！！！

```shell
./zkcluster_run.sh 'node01 node02 node03'  start  /hadoop/install/zookeeper-3.4.5-cdh5.14.2/bin/zkServer.sh
```

## hadoop格式化并启动

### 格式化namenode

初始化的目的就是为了hdfs的元数据信息的初始化。

命令

```shell
hdfs namenode -format
```

成功的标志：

```log
19/08/23 04:32:34 INFO namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   user = hadoop
STARTUP_MSG:   host = ......
STARTUP_MSG:   args = [-format]
STARTUP_MSG:   version = 2.6.0-cdh5.14.2
#显示格式化成功。。。
cdh5.14.2/hadoopDatas/namenodeDatas has been successfully formatted.
19/08/23 04:32:35 INFO common.Storage: Storage directory /hadoop/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/dfs/nn/edits has been successfully formatted.
19/08/23 04:32:35 INFO namenode.FSImageFormatProtobuf: Saving image file /hadoopadmin/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas/current/fsimage.ckpt_0000000000000000000 using no compression
19/08/23 04:32:35 INFO namenode.FSImageFormatProtobuf: Image file /hadoopadmin/install/hadoop-2.6.0-cdh5.14.2/hadoopDatas/namenodeDatas/current/fsimage.ckpt_0000000000000000000 of size 323 bytes saved in 0 seconds.
19/08/23 04:32:35 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0
19/08/23 04:32:35 INFO util.ExitUtil: Exiting with status 0
19/08/23 04:32:35 INFO namenode.NameNode: SHUTDOWN_MSG: 
#此处省略部分日志
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at .....
************************************************************/
```

如上步骤都顺利的话接下来就可以启动集群了！

### 启动集群

两种方式~

```shell
start-all.sh
# 不过这种方式官方已经不在推荐了
```

可以使用如下启动

```shell
# 启动hdfs
start-dfs.sh
# 启动yarn
start-yarn.sh
```

### 查看webUI

启动完成后可以通过webUI查看集群的信息，打开下边链接即可查看！！

[node01:50070](http://node01:50070)

## 常见问题

## 网卡配置



